## 지도학습

### 분류

미리 정의된, 가능성 있는 여러 **클래스 레이블** 중 하나를 예측하는 것

- 이진 분류(질문의 답이 예/아니오), 다중 분류(셋 이상의 클래스로 분류)



### 회귀

연속적인 숫자, 또는 프로그래밍 용어로 말하면 부동소수점수(수학 용어로는 실수)를 예측하는 것

출력 값에 연속성이 있는지 => 있다면 회귀



### 일반화

모델이 처음 보는 데이터에 대해 정확하게 예측할 수 있으면 이를 훈련 세트에서 테스트 세트로 **일반화** 되었다고 한다.



### 과대적합 / 과소 적합

**과대적합**은 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져 있어 새로운 데이터에 일반화되기 어려울 때 일어난다. => **많은 양의 데이터**를 통해 과대적합 없이 더 복잡한 모델을 만들 수 있다.

모델이 너무 간단하다면 **과소적합** 이라고 한다. 데이터의 면면과 다양성을 잡아내지 못할 것이고 훈련 세트에도 잘 맞지 않을 것임.

------

### k-최접근 이웃

- 가장 간단한 머신러닝 알고리즘.
- 훈련 데이터셋을 그냥 저장하는 것이 모델을 만드는 과정의 전부
- 새로운 데이터 포인트에  대해 예측할 땐 알고리즘이 훈련 데이터셋에서 가장 가까운 데이터 포인트, 즉 '최근접 이웃'을 찾음.
- 매개변수
  - `n_neighbors` : 가장 데이터 포인트를 정할 때 몇 개의 이웃으로 찾을지 이웃의 수를 정하는 매개변수이다. 여러 이웃 중 많은 비율을 차지하고 있는 레이블로 정하며, `n_neighbors` 의 값을 낮추면(이웃의 수가 적으면) 훈련 데이터에 대한 예측이 완벽해진다.(과대적합, 모델이 복잡해짐)
- k-NN 장점은 이해하기 매우 쉬운 모델이라는 점이다. 많이 조정하지 않아도 자주 좋은 성능을 발휘함. 더 복잡한 알고리즘을 적용해보기 전에 시도해볼만한 시작점이다.
- 데이터 전처리 과정이 중요. 수백 개 이상의 많은 특성을 가진 데이터셋에는 잘 작동하지 않으며, 특성값 대부분이 0 인 데이터셋과는 특히 잘 작동하지 않음. 현업에서 잘 쓰지 않는다. 

-----

## 선형 모델

입력 특성에 대한 **선형 함수**를 만들어 예측을 수행

회귀를 위한 선형 모델은 특성이 하나일 땐 직선, 두 개일 땐 평면이 되며, 더 높은 차원(특성이 더 많음)에서는 **초평면**이 되는 회귀 모델의 특징을 가지고 있다.



### 선형 회귀(최소제곱법) 

예측과 훈련 세트에 있는 타깃 y 사이의 **평균제곱오차**를 취소화하는 파라미터 w와 b를 찾는다.

선형 회귀는 매개변수가 없는 것이 장점이지만, 그래서 모델의 복잡도를 제어할 방법도 없다.



훈련 데이터와 테스터 사이의 성능 차이(train > test)는 모델이 과대적합되었다는 확실한 신호이므로 복잡도를 제어할 수 있는 모델을 사용해야함. 기본 선형 회귀 방식 대신 가장 널리 쓰이는 모델은 **리지 회귀** 이다.



### 리지 회귀

회귀를 위한 선형 모델. 최소적합법에서 사용한 것과 같은 예측 함수를 사용.

가중치(w) 선택은 훈련 데이터를 잘 예측하기 위해서 뿐만 아니라 추가 제약 조건을 만족시키기 위한 목적도 있다. => 가중치의 절댓값을 가능한 한 작게 만드는 것(w의 모든 윈소가 0에 가깝게 만드는 것)을 **규제**라 한다.

리지 회귀에서 사용하는 규제 방식을 **L2 규제**라고 한다.



Ridge는 모델을 단순하게(계수를 0에 가깝게) 해주고 훈련 세트에 대한 성능 사이를 절충할 수 있는 방법을 제공함. `alpha` 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지 지정함.

 `alpha`(기본값 alpha=1.0) 값을 높이면 계수를 0에 더 가깝게 만들어서 훈련 세트의 성능은 나빠지지만 일반화에는 도움을 줄 수 있다.

---------------

### 라소

리지 회귀에서와 같이 **라소**도 계수를 0에 가깝게 만든다. 하지만 **L1 규제 방식**을 사용한다.

L1 규제의 결과로 라소를 사용할 때 어떤 계수는 정말 0이 된다. 즉, 모델에서 완전히 제외되는 특성이 존재한다는 뜻이다. 어떻게 보면 **특성 선택 feature selection** 이 자동으로 이뤄진다고 볼 수 있다.

